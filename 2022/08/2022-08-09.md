# 2022. 08. 09.

## Elasticsearch(7.10)

#### 텍스트 분석 - 개요

#### 정규화(Normalization)

토큰화는 개별 텀에 일치시킬 수 있게 만들지만, 각 토큰은 여전히 문자 그대로 일치된다. 즉:

* 다른 텀에 일치하길 원할 수도 있음에도 `Quick`에 대한 검색은 `quick`에 일치하지 않는다.
* `fox`와 `foxes`는 같은 어근을 공유함에도 `foxes`에 대한 검색은 `fox`에 일치하지 않는다. 그 반대도 마찬가지이다.
* `jumps`는 `leaps`에 일치하지 않는다. 이들은 같은 어근을 공유하지는 않지만 유의어이고 비슷한 의미를 갖는다.

이러한 문제를 해결하기 위해 텍스트 분석은 토큰을 표준 형식으로 *정규화(normalize)*할 수 있다. 이는 검색 텀과 정확하게 일치하지 않지만 연관성이 있을 만큼 충분히 비슷한 토큰에도 일치하도록 한다. 예를 들어:

* `Quick`은 소문자로 변환할 수 있다: `quick`.
* `foxes`를 어근으로 *줄이(stemmed)*거나 제거할 수 있다: `fox`.
* `jump`와 `leap`는 유의어이므로 단일 단어로 인덱스할 수 있다: `jump`.

검색 텀이 의도대로 이 단어들에 일치하도록 하기 위해 쿼리 문자열에 같은 토큰화와 정규화 규칙을 적용할 수 있다. 예를 들어, `Foxes leap`는 `fox jump`에 대한 검색으로 정규화될 수 있다.