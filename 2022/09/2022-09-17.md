# 2022. 09. 17.

## Elasticsearch(7.10)

### 텍스트 분석 - 내장 분석기 레퍼런스

#### Stop 분석기

`stop` 분석기는 [`simple` 분석기][simple-analyzer]와 비슷하지만 불용어에 대한 지원을 더한다. 기본값은 `_english_` 불용어를 사용한다.

##### 예제 출력

```http
POST _analyze
{
  "analyzer": "stop",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

위 문장은 아래 텀을 만들어낸다:

```
[ quick, brown, foxes, jumped, over, lazy, dog, s, bone ]
```

##### 구성

`stop` 분석기는 다음 파라미터를 받는다:

| 파라미터         | 설명                                                         |
| ---------------- | ------------------------------------------------------------ |
| `stopwords`      | `_english_`와 같이 미리 정의된 불용어 혹은 불용어 목록 배열. 기본값은 `_english_`이다. |
| `stopwords_path` | 불용어 파일 경로. 이 경로는 Elasticsearch `config` 디렉터리에 대한 상대 경로이다. |

불용어 구성에 대한 더 자세한 내용은 [Stop 토큰 필터][stop-token-filter]를 참고하라.

##### 예제 구성

이 예제에서는 `stop` 분석기가 지정된 단어 목록을 불용어로 사용하도록 구성한다:

```http
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_stop_analyzer": {
          "type": "stop",
          "stopwords": ["the", "over"]
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_stop_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

위 예제는 다음 텀을 만들어낸다:

```
[ quick, brown, foxes, jumped, lazy, dog, s, bone ]
```

##### 정의

다음으로 구성된다:

**토큰화기**

- [Lower Case 토큰화기](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-lowercase-tokenizer.html)

**토큰 필터**

- [Stop 토큰 필터](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-stop-tokenfilter.html)

`stop` 분석기를 구성 파라미터 이상으로 커스터마이즈해야 한다면 이를 `custom` 분석기로 다시 만들어 주로 토큰 필터를 추가하여 변형해야 한다. 이렇게 내장 `stop` 분석기를 다시 만들어서 추가적인 커스터마이징의 시작점으로 사용할 수 있다:

```http
PUT /stop_example
{
  "settings": {
    "analysis": {
      "filter": {
        "english_stop": {
          "type":       "stop",
          "stopwords":  "_english_" // 1. 기본 불용어는 `stopwords`나 `stopwords_path` 파라미터로 오버라이드할 수 있다.
        }
      },
      "analyzer": {
        "rebuilt_stop": {
          "tokenizer": "lowercase",
          "filter": [
            "english_stop"          // 2. `english_stop` 뒤에 아무 토큰 필터나 추가할 수 있다.
          ]
        }
      }
    }
  }
}
```







[simple-analyzer]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-simple-analyzer.html
[stop-token-filter]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-stop-tokenfilter.html