# 2022. 09. 16.

## Elasticsearch(7.10)

### 텍스트 분석 - 내장 분석기 레퍼런스

#### Standard 분석기

`standard` 분석기는 아무것도 지정되지 않으면 사용되는 기본 분석기다. ([Unicode Standard Annex #29][unicode-reports-29]에 기술된 유니코드 텍스트 세그멘테이션 알고리즘을 기반으로 한)문법 기반 토큰화를 제공하며 대부분의 언어에 잘 동작한다.

##### 예제 출력

```http
POST _analyze
{
  "analyzer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

위 문장은 다음과 같은 텀을 만들어낸다:

```
[ the, 2, quick, brown, foxes, jumped, over, the, lazy, dog's, bone ]
```

##### 구성

`standard` 분석기는 다음 파라미터를 받는다:

| 파라미터           | 설명                                                         |
| ------------------ | ------------------------------------------------------------ |
| `max_token_length` | 최대 토큰 길이. 토큰이 이 길이를 초과하면 `max_token_length`마다 나눈다. 기본값은 `255`이다. |
| `stopwords`        | `_english_`와 같이 미리 정의된 불용어 목록이나 불용어 목록을 가진 배열. 기본값은 `_none_`이다. |
| `stopwords_path`   | 불용어 파일 경로.                                            |

불용어 구성에 관한 더 자세한 내용은 [Stop 토큰 필터][stop-token-filter]를 참고하라.

##### 예제 구성

이 예제에서는 (설명을 위해)`max_token_length`가 5인 `standard` 분석기를 구성하고 미리 정의된 English 불용어 목록을 사용하도록 구성한다:

```http
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_english_analyzer": {
          "type": "standard",
          "max_token_length": 5,
          "stopwords": "_english_"
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_english_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

위 예제는 다음 텀을 만들어낸다:

```
[ 2, quick, brown, foxes, jumpe, d, over, lazy, dog's, bone ]
```

##### 정의

`standard` 분석기는 다음으로 구성된다:

**토큰화기**

- [Standard 토큰화기](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-standard-tokenizer.html)

**토큰 필터**

- [Lower Case 토큰 필터](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-lowercase-tokenfilter.html)
- [Stop 토큰 필터](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-stop-tokenfilter.html) (기본값은 비활성화)

구성 파라미터 이상으로 `standard` 분석기를 커스터마이즈해야 한다면 이를 `custom` 분석기로 다시 만들고 주로 토큰 필터를 추가하는 방식으로 변형해야 한다. 내장 `standard` 분석기를 다시 만들어서 추가적인 커스터마이즈의 시작점으로 사용할 수 있다:

```http
PUT /standard_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "rebuilt_standard": {
          "tokenizer": "standard",
          "filter": [
            "lowercase"       // 1. `lowercase` 뒤에 아무 토큰 필터나 추가할 수 있다.
          ]
        }
      }
    }
  }
}
```



[unicode-reports-29]: https://unicode.org/reports/tr29/
[stop-token-filter]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-stop-tokenfilter.html