# 2022. 09. 14.

## Elasticsearch(7.10)

### 텍스트 분석 - 내장 분석기 레퍼런스

#### Pattern 분석기

##### 구성

`pattern` 분석기는 다음 파라미터를 받는다:

| 파라미터         | 설명                                                         |
| ---------------- | ------------------------------------------------------------ |
| `pattern`        | [Java 정규 표현식](https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html), 기본값은 `\W+`이다. |
| `flags`          | Java 정규 표현식 [플래그](https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#field.summary). 플래그는 파이프(`|`)로 구분한다, eg `"CASE_INSENSITIVE|COMMENTS"`. |
| `lowercase`      | 텀을 소문자로 변환해야 하는지를 나타낸다. 기본값은 `true`이다. |
| `stopwords`      | `_english_`와 같이 미리 정의된 불용어 목록 혹은 불용어 목록 배열. 기본값은 `_none_`이다. |
| `stopwords_path` | 불용어가 포함된 파일 경로.                                   |

불용어 구성에 관한 더 자세한 내용은 [Stop 토큰 필터][stop-token-filter]를 참고하라.

##### 예제 구성

이 예제에서는 `pattern` 분석기가 이메일 주소들을 단어가 아닌 문자나 언더스코어(`\W|_`)로 나누고 그 결과를 소문자로 변환하도록 구성한다:

```http
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_email_analyzer": {
          "type":      "pattern",
          "pattern":   "\\W|_", // 1. 패턴을 JSON 문자열로 나타낼 때 이스케이프를 위해 백슬래시가 필요하다.
          "lowercase": true
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_email_analyzer",
  "text": "John_Smith@foo-bar.com"
}
```

위 예제는 다음 텀을 만들어낸다:

```
[ john, smith, foo, bar, com ]
```

###### CamelCase 토큰화기

다음은 카멜 케이스(CamelCase) 텍스트를 토큰으로 분리하는 더 복잡한 예제이다:

```http
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "camel": {
          "type": "pattern",
          "pattern": "([^\\p{L}\\d]+)|(?<=\\D)(?=\\d)|(?<=\\d)(?=\\D)|(?<=[\\p{L}&&[^\\p{Lu}]])(?=\\p{Lu})|(?<=\\p{Lu})(?=\\p{Lu}[\\p{L}&&[^\\p{Lu}]])"
        }
      }
    }
  }
}

GET my-index-000001/_analyze
{
  "analyzer": "camel",
  "text": "MooseX::FTPClass2_beta"
}
```

위 예제는 다음 텀을 만들어낸다:

```
[ moose, x, ftp, class, 2, beta ]
```

위 정규식은 아래처럼 이해하면 쉽다:

```
  ([^\p{L}\d]+)                 # swallow non letters and numbers,
| (?<=\D)(?=\d)                 # or non-number followed by number,
| (?<=\d)(?=\D)                 # or number followed by non-number,
| (?<=[ \p{L} && [^\p{Lu}]])    # or lower case
  (?=\p{Lu})                    #   followed by upper case,
| (?<=\p{Lu})                   # or upper case
  (?=\p{Lu}                     #   followed by upper case
    [\p{L}&&[^\p{Lu}]]          #   then lower case
  )
```

##### 정의

`pattern` 분석기는 다음으로 구성된다:

**토큰화기**

[Pattern Tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-pattern-tokenizer.html)

**토큰 필터**

* [Lower Case Token Filter](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-lowercase-tokenfilter.html)
* [Stop Token Filter](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-stop-tokenfilter.html) (기본값은 비활성화)

`pattern` 분석기를 구성 파라미터 이상으로 커스터마이즈해야 한다면 이를 `custom` 분석기로 다시 만들고 주로 토큰 필터를 추가하는 방식으로 변형하면 된다. 이렇게 내장 `pattern` 분석기를 다시 만들어서 추가적인 커스터마이즈를 위한 시작점으로 사용할 수 있다:

```http
PUT /pattern_example
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "split_on_non_word": {
          "type":       "pattern",
          "pattern":    "\\W+" // 1. 기본 패턴은 단어가 아닌 문자로 나누는 `\W+`지만 바꿀 수 있다.
        }
      },
      "analyzer": {
        "rebuilt_pattern": {
          "tokenizer": "split_on_non_word",
          "filter": [
            "lowercase"       // 2. `lowecase` 뒤에 다른 토큰 필터를 추가할 수 있다.
          ]
        }
      }
    }
  }
}
```



[stop-token-filter]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-stop-tokenfilter.html