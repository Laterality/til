# 2022. 11. 11.

## Elasticsearch(7.10)

### 텍스트 분석 - 토큰 필터 레퍼런스

#### Remove duplicate 토큰 필터

같은 위치(position)에서 중복된 토큰을 제거한다.

`remove_duplicates` 필터는 Lucene의 [RemoveDuplicatesTokenFilter][lucene-remove-duplicates-token-filter]를 사용한다.

##### 예제

`remove_duplicates` 필터가 어떻게 동작하는지 살펴보기 위해 먼저 같은 위치에 중복된 토큰을 가진 토큰 스트림을 만들어야 한다.

다음 [분석 API][analyze-api]는 [`keyword_repeat`][keyword-repeat-token-filter]와 [`stemmer`][stemmer-token-filter] 필터를 사용해 `jumping dog`에 대해 스테밍된 토큰과 스테밍되지 않은 토큰을 만든다.

```http
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "keyword_repeat",
    "stemmer"
  ],
  "text": "jumping dog"
}
```

API는 다음 응답을 반환한다. 위치 `1`에 `dog` 토큰이 중복됐다.

```json
{
  "tokens": [
    {
      "token": "jumping",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "jump",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "dog",
      "start_offset": 8,
      "end_offset": 11,
      "type": "word",
      "position": 1
    },
    {
      "token": "dog",
      "start_offset": 8,
      "end_offset": 11,
      "type": "word",
      "position": 1
    }
  ]
}
```

중복된 `dog` 토큰 중 하나를 제거하기 위해 이전 분석 API 요청에 `remove_duplicates` 필터를 추가한다.

```http
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "keyword_repeat",
    "stemmer",
    "remove_duplicates"
  ],
  "text": "jumping dog"
}
```

API는 다음 응답을 반환한다. 이제 위치 `1`에는 하나의 `dog` 토큰만 남아있다.

```json
{
  "tokens": [
    {
      "token": "jumping",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "jump",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "dog",
      "start_offset": 8,
      "end_offset": 11,
      "type": "word",
      "position": 1
    }
  ]
}
```

##### 분석기에 추가

다음 [인덱스 생성 API][create-index-api] 요청은 `remove_duplicates` 필터를 사용해 새 [커스텀 분석기][custom-analyzer]를 구성한다.

커스텀 분석기는 `keyword_repeat`과 `stemmer` 필터를 사용해 스트림에서 각 토큰의 스테밍된 버전과 스테밍되지 않은 버전을 생성한다. 그 다음 `remove_duplicates` 필터가 같은 위치에 중복된 토큰을 제거한다.

```http
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "keyword_repeat",
            "stemmer",
            "remove_duplicates"
          ]
        }
      }
    }
  }
}
```



[lucene-remove-duplicates-token-filter]: https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.html
[analyze-api]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-analyze.html
[keyword-repeat-token-filter]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-keyword-repeat-tokenfilter.html
[stemmer-token-filter]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-stemmer-tokenfilter.html
[create-index-api]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-create-index.html
[custom-analyzer]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-custom-analyzer.html