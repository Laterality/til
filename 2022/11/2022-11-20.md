# 2022. 11. 20.

## Elasticsearch(7.10)

### 텍스트 분석 - 토큰 필터 레퍼런스

#### Synonym graph 토큰 필터

`synonym_graph` 토큰 필터는 분석 과정에서 다중 단어 동의어(multi-word synonym)을 포함한 동의어를 쉽게 처리할 수 있다.

다중 단어 동의어를 제대로 처리하기 위해 이 필터는 처리 과정에서 [그래프 토큰 스트림][token-graph]을 만든다. 이 주제에 관한 더 자세한 정보와 다양한 복잡성에 대해서는 블로그 글 [Lucene의 TokenStream은 사실 그래프다][lucene-token-stream-blog-post]를 참고하라.

> 이 토큰 필터는 검색 분석기의 일부로만 사용되기 위해 설계됐다. 인덱싱 과정에서 동의어를 적용하려면 표준 [synonym 토큰 필터][synonym-token-filter]를 사용하라.

동의어는 구성 파일을 사용해 구성한다. 아래는 예제이다:

```http
PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "search_synonyms": {
            "tokenizer": "whitespace",
            "filter": [ "graph_synonyms" ]
          }
        },
        "filter": {
          "graph_synonyms": {
            "type": "synonym_graph",
            "synonyms_path": "analysis/synonym.txt"
          }
        }
      }
    }
  }
}
```

위에서는 (`config` 위치의 상대 경로)`analysis/synonym.txt`를 경로로 갖는 `synonym` 필터를 구성한다. 그 다음 `synonym` 분석기를 필터와 함께 구성한다.

이 필터는 체인에서 앞에 위치한 토큰화기와 토큰 필터와 함께 동의어를 토큰화한다.

추가 설정은 다음과 같다:

* `expand`(기본값은 `ture`)

* `lenient`(기본값은 `false`). `true`이면 동의어 구성을 파싱할 때 예외가 발생한 경우 이를 무시한다. 파싱할 수 없는 동의어 규칙들만 무시된다는 점이 중요하다. 예를 들어 다음 요청을 보자:

  ```http
  PUT /test_index
  {
    "settings": {
      "index": {
        "analysis": {
          "analyzer": {
            "synonym": {
              "tokenizer": "standard",
              "filter": [ "my_stop", "synonym_graph" ]
            }
          },
          "filter": {
            "my_stop": {
              "type": "stop",
              "stopwords": [ "bar" ]
            },
            "synonym_graph": {
              "type": "synonym_graph",
              "lenient": true,
              "synonyms": [ "foo, bar => baz" ]
            }
          }
        }
      }
    }
  }
  ```

  위 요청에서 단어 `bar`는 넘어가지만 매핑 `foo => baz`는 여전히 추가된다. 하지만 추가된 매핑이 `foo, baz => bar`라면 동의어 목록에 아무것도 추가되지 않는다. 불용어이기 때문에 매핑에 대한 대상 단어 자체가 제거되기 때문이다. 비슷하게, `expand=false`이면 대상 매핑은 첫 번째 단어이기 때문에 매핑이 "bar, foo, baz"이고 `expand`가 `false`로 설정되면 어떤 매핑도 추가되지 않는다. 하지만 `expand=true`이면 추가되는 매핑은 `foo, baz => foo, baz`와 동등할 것이다. 즉, 불용어를 제외한 모든 매핑이 된다.

##### `tokenizer`와 `ignore_case`는 폐기 예정(deprecated)

`tokenizer` 파라미터는 동의어를 토큰화할 때 사용될 토큰화기를 조정한다.

이 파라미터는 6.0 이전에 생성된 인덱스와의 하위 호환성을 위한 것이다. `ignore_case` 파라미터는 `tokenizer` 파라미터와 함께일 때만 동작한다.

두 가지 동의어 형식이 지원된다: Solr, WordNet

##### Solr 동의어

다음은 파일의 샘플 형식이다:

```
# Blank lines and lines starting with pound are comments.

# Explicit mappings match any token sequence on the LHS of "=>"
# and replace with all alternatives on the RHS.  These types of mappings
# ignore the expand parameter in the schema.
# Examples:
i-pod, i pod => ipod
sea biscuit, sea biscit => seabiscuit

# Equivalent synonyms may be separated with commas and give
# no explicit mapping.  In this case the mapping behavior will
# be taken from the expand parameter in the schema.  This allows
# the same synonym file to be used in different synonym handling strategies.
# Examples:
ipod, i-pod, i pod
foozball , foosball
universe , cosmos
lol, laughing out loud

# If expand==true, "ipod, i-pod, i pod" is equivalent
# to the explicit mapping:
ipod, i-pod, i pod => ipod, i-pod, i pod
# If expand==false, "ipod, i-pod, i pod" is equivalent
# to the explicit mapping:
ipod, i-pod, i pod => ipod

# Multiple synonym mapping entries are merged.
foo => foo bar
foo => baz
# is equivalent to
foo => foo bar, baz
```

구성 파일의 필터에 대한 동의어를 직접 정의할 수도 있다(`synonyms_path` 대신 `synonyms`를 사용한다는 점을 참고하라):

```http
PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "filter": {
          "synonym": {
            "type": "synonym_graph",
            "synonyms": [
              "lol, laughing out loud",
              "universe, cosmos"
            ]
          }
        }
      }
    }
  }
}
```

하지만 대량의 동의어 집합은 `synonyms_path`를 사용해 파일로 정의하는 것을 권장한다. 이를 인라인으로 정의하면 클러스터 크기를 불필요하게 키우기 때문이다.

##### WordNet 동의어

`format`을 사용해 [WordNet][wordnet] 형식을 기반으로 한 동의어를 선언할 수 있다:

```http
PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "filter": {
          "synonym": {
            "type": "synonym",
            "format": "wordnet",
            "synonyms": [
              "s(100000001,1,'abstain',v,1,0).",
              "s(100000001,2,'refrain',v,1,0).",
              "s(100000001,3,'desist',v,1,0)."
            ]
          }
        }
      }
    }
  }
}
```

`synonyms_path`를 사용해 WordNet 동의어를 파일에 정의하는 것도 지원된다.

##### 동의어 파일 파싱

Elasticsearch는 토큰화기 체인에서 동의어 필터보다 앞에 위치한 토큰 필터를 사용해 동의어 파일의 엔트리를 파싱한다. 따라서, 예를 들어 동의어 필터가 스테머 뒤에 위치하면 동의어 엔트리에도 스테머가 적용될 것이다. 동의어 맵의 엔트리는 누적된(stacked) 위치를 가질 수 없기 때문에 일부 토큰 필터는 여기서 문제가 발생한다. 토큰의 여러 버전을 만들어내는 토큰 필터는 동의어를 파싱할 때 어떤 버전의 토큰을 내보낼지 선택할 것이다. e.g. `asciifolding`은 토큰의 폴딩된 버전만 만들어낼 것이다. `multiplexer`, `word_delimiter_graph` 혹은 `ngram`과 같은 다른 필터는 오류를 던질 것이다.

다중 토큰 필터와 동의어 필터를 모두 포함하는 분석기를 구축해야 한다면 한 분기에는 다중 토큰 필터를 사용하고 다른 분기에는 동의어 필터를 사용하는 [multiplexer][multiplexer-token-filter] 필터를 고려하라.

> 동의어 규칙은 체인에서 이후에 (`stop` 필터 등에 의해)제거되는 단어는 포함하지 않는 것이 좋다. 동의어 규칙에서 텀을 제거하면 쿼리 시점에 일치하지 않게 된다.



[token-graph]: 처리과정에서
[lucene-token-stream-blog-post]: http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html
[synonym-token-filter]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-synonym-tokenfilter.html
[wordnet]: https://wordnet.princeton.edu/
[multiplexer-token-filter]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-multiplexer-tokenfilter.html