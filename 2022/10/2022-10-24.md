# 2022. 10. 24.

## Elasticsearch(7.10)

### 텍스트 분석 - 토큰 필터 레퍼런스

#### Hyphenation decompounder 토큰 필터

XML 기반 하이픈 사용 규칙 패턴을 사용해 복합어(compund word)에서 잠재적인 부단어(subword)를 찾는다. 그 다음 이 부단어들을 지정된 단어 목록에 있는지 확인한다. 리스트에 없는 부단어들은 토큰 출력에서 제외된다.

이 필터는 독일어를 위해 구축된 Lucene의 [HyphenationCompoundWordTokenFilter][lucene-hyphenation-compound-word-token-filter]를 사용한다.

##### 예제

다음 [분석 API][analyze-api] 요청은 `hyphenation_decompounder` 필터를 사용해 `analysis/hyphenation_patterns.xml` 파일의 독일어 하이픈 사용 규칙 패턴을 기반으로 `Kaffeetasse`에서 부단어를 찾는다. 그 다음 필터는 부단어들을 지정된 단어 목록 `kaffee`, `zucker`, `tasse`에 있는지 확인한다.

```http
GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "hyphenation_decompounder",
      "hyphenation_patterns_path": "analysis/hyphenation_patterns.xml",
      "word_list": ["Kaffee", "zucker", "tasse"]
    }
  ],
  "text": "Kaffeetasse"
}
```

필터는 다음과 같은 토큰을 만들어낸다:

```
[ Kaffeetasse, Kaffee, tasse ]
```

##### 구성 가능한 파라미터

**`hyphenation_patterns_path`**

(Required, string) Apache FOP (Formatting Objects Processor) XML 하이픈 사용 규칙 패턴 파일의 경로.

이 경로는 절대 경로이거나 `config` 위치에 대한 상대 경로여야 한다. FOP v1.2와 호환되는 파일만 지원된다.

FOP XML 하이픈 사용 규칙 파일의 예시는 다음을 참고하라:

- [Objects For Formatting Objects (OFFO) Sourceforge project](http://offo.sourceforge.net/#FOP+XML+Hyphenation+Patterns)
- [offo-hyphenation_v1.2.zip direct download](https://sourceforge.net/projects/offo/files/offo-hyphenation/1.2/offo-hyphenation_v1.2.zip/download) (v2.0 및 그 이상 버전의 하이픈 사용 규칙 패턴 파일은 지원되지 않는다)

**`word_list`**

(Required*, array of strings) 부단어 목록. 하이픈 사용 규칙 패턴을 사용해 검출됐지만 이 목록에 없는 부단어는 토큰 출력에서 제외된다.

구현하기 전에 [`dictionary_decompounder`](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-dict-decomp-tokenfilter.html) 필터를 사용해 단어 목록의 품질을 테스트할 수 있다.

이 파라미터나 `word_list_path` 중 하나는 명시돼야 한다.

**`word_list_path`**

(Required*, string) 부단어 목록을 가진 파일 경로. 하이픈 사용 규칙 패턴을 사용해 검출됐지만 이 목록에 없는 부단어는 토큰 출력에서 제외된다.

이 경로는 절대 경로이거나 `config` 위치에 대한 상대 경로여야 하며 UTF-8로 인코딩돼야 한다. 파일의 각 토큰은 개행으로 구분돼야 한다.

구현하기 전에 [`dictionary_decompounder`](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-dict-decomp-tokenfilter.html) 필터를 사용해 단어 목록의 품질을 테스트할 수 있다.

이 파라미터나 `word_list` 중 하나는 명시돼야 한다.

**`max_subword_size`**

(Optional, integer) 최대 부단어 문자 길이. 이보다 긴 부단어 토큰은 출력에서 제외된다. 기본값은 `15`이다.

**`min_subword_size`**

(Optional, integer) 최소 부단어 문자 길이. 이보다 짧은 부단어 토큰은 출력에서 제외된다. 기본값은 `2`이다.

**`min_word_size`**

(Optional, integer) 최소 단어 문자 길이. 이보다 짧은 단어 토큰은 출력에서 제외된다. 기본값은 `5`이다.

**`only_longest_match`**

(Optional, Boolean) `true`이면 일치하는 부단어 중 가장 긴 것만 갖는다. 기본값은 `false`이다.

##### 커스터마이즈해 분석기에 추가

`hyphenation_decopounder` 필터를 커스터마이즈하려면 이를 복제해 새 커스텀 토큰 필터를 만들면 된다. 구성 가능한 파라미터를 사용해 필터를 변형할 수 있다.

예를 들어, 다음 [인덱스 생성 API][create-index-api] 요청은 커스텀 `hyphenation_decompounder` 필터를 사용해 새 [커스텀 분석기][custom-analyzer]를 만든다.

이 커스텀 `hyphenation_decompounder` 필터는 `analysis/hyphenation_patterns.xml` 파일의 하이픈 사용 규칙 패턴을 기반으로 부단어를 찾는다. 그 다음 부단어를 `analysis/example_word_list.txt` 파일에 명시된 단어 목록에 존재하는지 검사한다. 22자를 초과하는 부단어는 토큰 출력에서 제외된다.

```http
PUT hyphenation_decompound_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_hyphenation_decompound": {
          "tokenizer": "standard",
          "filter": [ "22_char_hyphenation_decompound" ]
        }
      },
      "filter": {
        "22_char_hyphenation_decompound": {
          "type": "hyphenation_decompounder",
          "word_list_path": "analysis/example_word_list.txt",
          "hyphenation_patterns_path": "analysis/hyphenation_patterns.xml",
          "max_subword_size": 22
        }
      }
    }
  }
}
```



[lucene-hyphenation-compound-word-token-filter]: https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilter.html
[analyze-api]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-analyze.html
[create-index-api]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-create-index.html
[custom-analyzer]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-custom-analyzer.html