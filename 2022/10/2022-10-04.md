# 2022. 10. 04.

## Elasticsearch(7.10)

### 텍스트 분석 - 토큰화기 레퍼런스

#### Standard 토큰화기

`standard` 토큰화기는 ([Unicode Standard Annex #29][unicode-standard-annex-29]에 명시된 유니코드 텍스트 세그멘테이션 알고리즘에 기반한)문법(grammar) 기반 토큰화를 제공하고 대부분의 언어에서 잘 동작한다.

##### 예제 출력

```http
POST _analyze
{
  "tokenizer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

위 문장은 다음 텀을 만들어낸다:

```
[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]
```

##### 구성

`standard` 토큰화기는 다음 파라미터를 받는다:

| 파라미터           | 설명                                                         |
| ------------------ | ------------------------------------------------------------ |
| `max_token_length` | 최대 토큰 길이. 토큰이 이 길이를 초과하면 `max_token_length`마다 나눈다. 기본값은 `255`이다. |

##### 예제 구성

이 예제에서는 `standard` 토큰화기가 (설명을 위해)5의 `max_token_length`를 갖도록 구성한다:

```http
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "standard",
          "max_token_length": 5
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

위 예제는 다음 텀을 만들어낸다:

```
[ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]
```



[unicode-standard-annex-29]: https://unicode.org/reports/tr29/