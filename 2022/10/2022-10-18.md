# 2022. 10. 18.

## Elasticsearch(7.10)

### 텍스트 분석 - 토큰 필터 레퍼런스

#### Dictionary decompunder 토큰 필터

> 대부분의 경우 이 필터 대신 더 빠른 [`hyphenation_decompunder`][hyphenation-decompunder]를 사용할 것을 권장한다. 하지만 `hyphenation_decompunder` 필터에 구현하기 전에 단어 목록의 품질을 검사하는 데 `dictionary_decompunder`를 사용할 수 있다.

지정된 단어 목록을 사용해 무작위 대입으로 복합어(compund words)에서 부단어(subwords)를 찾는다. 만일 찾은 경우 부단어가 토큰 출력에 포함된다.

이 필터는 독일어를 위해 구축된 Lucene의 [DictionaryCompoundWordTokenFilter][lucene-dictionary-compound-word-token-filter]를 사용한다.

##### 분석

다음 [분석 API][analyze-api] 요청은 `dictionary_decompounder` 필터를 사용해 `Donaudampfschiff`에서 부단어를 찾는다. 필터는 부단어들을 지정된 단어 목록`Donau`, `dampf`, `meer`,  `schiff`에 대해 검사한다.

```http
GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "dictionary_decompounder",
      "word_list": ["Donau", "dampf", "meer", "schiff"]
    }
  ],
  "text": "Donaudampfschiff"
}
```

필터는 다음과 같은 토큰을 만들어낸다:

```
[ Donaudampfschiff, Donau, dampf, schiff ]
```

##### 구성 가능한 파라미터

**`word_list`**

(필수*, 문자열 배열) 토큰 스트림에서 조회할 부단어 목록, 일치하는 부단어가 존재하면 토큰 출력에 포함된다. 이 파라미터와 `word_list_path` 중 하나는 명시해야 한다.

**`word_list_path`**

(필수*, string) 부단어 목록을 가진 파일 경로. 일치하는 부단어가 존재하면 토큰 출력에 포함된다. 이 경로는 절대경로거나 `config` 위치에 대한 상대경로여야 하며, 파일은 UTF-8로 인코딩돼야 한다. 파일의 각 토큰은 개행으로 구분돼야 한다. 이 파라미터와 `word_list` 중 하나는 명시해야 한다.

**`max_subword_size`**

(선택, integer) 최대 부단어 문자 길이. 이보다 더 긴 부단어 토큰은 출력에서 제외된다. 기본값은 `15`이다.

**`min_subword_size`**

(선택, integer) 최소 부단어 문자 길이. 이보다 더 짧은 부단어 토큰은 출력에서 제외된다. 기본값은 `2`이다.

**`min_word_size`**

(선택, integer) 최소 단어 문자 길이. 이보다 더 짧은 단어 토큰은 출력에서 제외된다. 기본값은 `5`이다.

**`only_longest_match`**

(선택, Boolean) `true`이면 일치하는 가장 긴 부단어만 포함한다. 기본값은 `false`이다.

##### 커스터마이즈해 분석기에 추가

`dictionary_decompounder` 필터를 커스터마이즈하려면 이를 복제해 새 커스텀 토큰 필터를 만들면 된다. 구성 가능한 파라미터를 사용해 변형할 수 있다.

예를 들어, 다음 [인덱스 생성 API][create-index-api] 요청은 커스텀 `dictionary_decompounder` 필터를 사용해 새 [커스텀 분석기][custom-analyzer]를 구성한다.

커스텀 `dictionary_decompounder` 필터는 `analysis/example_word_list.txt` 파일에서 부단어를 찾는다. 22자를 초과하는 부단어는 토큰 출력에서 제외된다.

```http
PUT dictionary_decompound_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_dictionary_decompound": {
          "tokenizer": "standard",
          "filter": [ "22_char_dictionary_decompound" ]
        }
      },
      "filter": {
        "22_char_dictionary_decompound": {
          "type": "dictionary_decompounder",
          "word_list_path": "analysis/example_word_list.txt",
          "max_subword_size": 22
        }
      }
    }
  }
}
```





[hyphenation-decompunder]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-hyp-decomp-tokenfilter.html
[lucene-dictionary-compound-word-token-filter]: https://lucene.apache.org/core/8_7_0/analyzers-common/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.html
[analyze-api]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-analyze.html
[create-index-api]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/indices-create-index.html
[custom-analyzer]: https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-custom-analyzer.html